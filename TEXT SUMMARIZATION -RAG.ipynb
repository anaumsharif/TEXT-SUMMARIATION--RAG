{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNwm6YBGjcChjtxhUdyornQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"6f6641631ccb4515a4e57b3f6f45ef14":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_47efdae5998a43aebcf154abc26fd862","IPY_MODEL_f6d0982fa594453b987c20bb95a24472","IPY_MODEL_6bf12d71ae804a04b6e385891777cc6e"],"layout":"IPY_MODEL_078ed8146283467db43eb5cf3d99e4c0"}},"47efdae5998a43aebcf154abc26fd862":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4225b7bd84964d94982cf81ab8bcbfcb","placeholder":"​","style":"IPY_MODEL_87388a23328a42b1bc8fdc3b816ae945","value":"config.json: 100%"}},"f6d0982fa594453b987c20bb95a24472":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f26ad06a98aa4efba7807d46f59d78c9","max":1429,"min":0,"orientation":"horizontal","style":"IPY_MODEL_eb2fde40ad784cd0999e74cce385e9a3","value":1429}},"6bf12d71ae804a04b6e385891777cc6e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ee5e92cf0246489e94fae6057d771a02","placeholder":"​","style":"IPY_MODEL_e5a23c3c46e94eeab18a44b993c6cfe2","value":" 1.43k/1.43k [00:00&lt;00:00, 23.1kB/s]"}},"078ed8146283467db43eb5cf3d99e4c0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4225b7bd84964d94982cf81ab8bcbfcb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"87388a23328a42b1bc8fdc3b816ae945":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f26ad06a98aa4efba7807d46f59d78c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb2fde40ad784cd0999e74cce385e9a3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ee5e92cf0246489e94fae6057d771a02":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e5a23c3c46e94eeab18a44b993c6cfe2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bc110421adb54b419938485a68b6b992":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2cffea010e774fe6b85bce83951476d6","IPY_MODEL_7f21468929eb439bb37036081dc106b6","IPY_MODEL_4b65a4a950c3445d9110f28998760b20"],"layout":"IPY_MODEL_24ed4e28989c4625aec5d4e88bec4fbd"}},"2cffea010e774fe6b85bce83951476d6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_22b0261d02b6413197852259c7ddc29f","placeholder":"​","style":"IPY_MODEL_f64582a4e9424b7f98ab14c24cafd72d","value":"model.safetensors: 100%"}},"7f21468929eb439bb37036081dc106b6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d5291805a50a49beb22885d5d83f64d6","max":2444578688,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7eee3028cac744c9a92e743c24784d14","value":2444578688}},"4b65a4a950c3445d9110f28998760b20":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9019d0a597e74ec598435793e4704496","placeholder":"​","style":"IPY_MODEL_ce3dbd5d580142878f6946d981dbae63","value":" 2.44G/2.44G [00:25&lt;00:00, 94.1MB/s]"}},"24ed4e28989c4625aec5d4e88bec4fbd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"22b0261d02b6413197852259c7ddc29f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f64582a4e9424b7f98ab14c24cafd72d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d5291805a50a49beb22885d5d83f64d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7eee3028cac744c9a92e743c24784d14":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9019d0a597e74ec598435793e4704496":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ce3dbd5d580142878f6946d981dbae63":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"35d7fd71c90747cfba36977cde0ae5a4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6c1dab3e72be4439b97b60c132b07e25","IPY_MODEL_3f93ecf8e6e34156b1bf0fb4fc069060","IPY_MODEL_68c1c8a8cf5a4360b3b3517a6ac6ff9a"],"layout":"IPY_MODEL_cf5b072ad5f948e095dfa044eebadc41"}},"6c1dab3e72be4439b97b60c132b07e25":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aaaccd87df0c4a479feedc4c04a8b834","placeholder":"​","style":"IPY_MODEL_814c453360d54b54b768f4939386d3e2","value":"generation_config.json: 100%"}},"3f93ecf8e6e34156b1bf0fb4fc069060":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d04ab1227587484f97e448d05b1085f6","max":261,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e080548c8b33464884bcbf2d76df5dd9","value":261}},"68c1c8a8cf5a4360b3b3517a6ac6ff9a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_09d9a411b3b84669ac8ce8909d0b50a2","placeholder":"​","style":"IPY_MODEL_554cd170af4a4fbdbcd5b24c32a8cea3","value":" 261/261 [00:00&lt;00:00, 11.3kB/s]"}},"cf5b072ad5f948e095dfa044eebadc41":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aaaccd87df0c4a479feedc4c04a8b834":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"814c453360d54b54b768f4939386d3e2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d04ab1227587484f97e448d05b1085f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e080548c8b33464884bcbf2d76df5dd9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"09d9a411b3b84669ac8ce8909d0b50a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"554cd170af4a4fbdbcd5b24c32a8cea3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"60178746f4f641d18e15e34ffff5624c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6871e04fc85e4dd7909fac8f7fdf09b2","IPY_MODEL_9e528916597a4749987bcb2e00b11c46","IPY_MODEL_f470760985774786afdf70938ad49105"],"layout":"IPY_MODEL_30f57fb044cb483f90526a86832d76b9"}},"6871e04fc85e4dd7909fac8f7fdf09b2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ad5ecff80614815b678f251ca433507","placeholder":"​","style":"IPY_MODEL_e486fb0c065d49d0949f99c441f4a77f","value":"tokenizer_config.json: 100%"}},"9e528916597a4749987bcb2e00b11c46":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2ba7c61991da4369a579b14ec5d542ab","max":529,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9a61363cb8fa4c2c83c675a6f5de98cc","value":529}},"f470760985774786afdf70938ad49105":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_10ccc27d7d4148d980fbebfe58bfa726","placeholder":"​","style":"IPY_MODEL_05f9f3e6c23d4b3bb5d6d2af2715f661","value":" 529/529 [00:00&lt;00:00, 25.3kB/s]"}},"30f57fb044cb483f90526a86832d76b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ad5ecff80614815b678f251ca433507":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e486fb0c065d49d0949f99c441f4a77f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2ba7c61991da4369a579b14ec5d542ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9a61363cb8fa4c2c83c675a6f5de98cc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"10ccc27d7d4148d980fbebfe58bfa726":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"05f9f3e6c23d4b3bb5d6d2af2715f661":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d1d6b39e236b4308abb148f9d543881c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d3f7c0ea641d41c4a4e100c0d3d529f9","IPY_MODEL_b7ede9c7c76c402890dd403c2e089aee","IPY_MODEL_f880d87f7ee04af197d304069864b9bd"],"layout":"IPY_MODEL_ff60ee85e13f45e59ca828a34a1d10a7"}},"d3f7c0ea641d41c4a4e100c0d3d529f9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_08d64eb7d53141a0a55abf22c9773a95","placeholder":"​","style":"IPY_MODEL_9b12961f9f8c4ca6a7191c2db197a4a5","value":"sentencepiece.bpe.model: 100%"}},"b7ede9c7c76c402890dd403c2e089aee":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_88e733d4797145acaa2aedf96c837e2e","max":5069051,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5c0380c2ff654aa6af8686717ffc617d","value":5069051}},"f880d87f7ee04af197d304069864b9bd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_961ca24821fb44439e12ee3e2846fc67","placeholder":"​","style":"IPY_MODEL_f12296c8df59404884aa1bf869fa6587","value":" 5.07M/5.07M [00:00&lt;00:00, 19.8MB/s]"}},"ff60ee85e13f45e59ca828a34a1d10a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08d64eb7d53141a0a55abf22c9773a95":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9b12961f9f8c4ca6a7191c2db197a4a5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"88e733d4797145acaa2aedf96c837e2e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c0380c2ff654aa6af8686717ffc617d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"961ca24821fb44439e12ee3e2846fc67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f12296c8df59404884aa1bf869fa6587":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8c3d9ea08b2a4561b576d56695bc5a2e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e8452a64b8a94619a2b84cc60770b682","IPY_MODEL_9a8f994b778249f9bf1434b5879aee29","IPY_MODEL_ab69c843dc78446fa1529db94897d3be"],"layout":"IPY_MODEL_61db3c657a924b259c4b8897e5cf877c"}},"e8452a64b8a94619a2b84cc60770b682":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_267350bd5d7c4b0bb9af36950a714076","placeholder":"​","style":"IPY_MODEL_91d22b361ac14e8cadbb9d199695c454","value":"special_tokens_map.json: 100%"}},"9a8f994b778249f9bf1434b5879aee29":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_56ad6a91e88847a2a33e50473bbc0d31","max":649,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7def6fcca8f14905a103a824b7928502","value":649}},"ab69c843dc78446fa1529db94897d3be":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_668d27875b684b0f833c023f4d9d0212","placeholder":"​","style":"IPY_MODEL_7d38d8024fb54c1f94b20fd0009b8760","value":" 649/649 [00:00&lt;00:00, 44.3kB/s]"}},"61db3c657a924b259c4b8897e5cf877c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"267350bd5d7c4b0bb9af36950a714076":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91d22b361ac14e8cadbb9d199695c454":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"56ad6a91e88847a2a33e50473bbc0d31":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7def6fcca8f14905a103a824b7928502":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"668d27875b684b0f833c023f4d9d0212":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d38d8024fb54c1f94b20fd0009b8760":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"fELDU-1l0aXw","executionInfo":{"status":"ok","timestamp":1708279215175,"user_tz":-330,"elapsed":2,"user":{"displayName":"alina shaikh","userId":"07427893967666901792"}}},"outputs":[],"source":["import os\n","open_api_key=\"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\"\n","os.environ[\"OPENAI_API_KEY\"]=open_api_key"]},{"cell_type":"code","source":["! pip install langchain"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qHciJJo90qGl","executionInfo":{"status":"ok","timestamp":1708279229361,"user_tz":-330,"elapsed":13490,"user":{"displayName":"alina shaikh","userId":"07427893967666901792"}},"outputId":"6b043b0f-9dd9-4317-d1eb-3db8c142fef2"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting langchain\n","  Downloading langchain-0.1.7-py3-none-any.whl (815 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.9/815.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.27)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n","Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n","Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n","  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n","Collecting jsonpatch<2.0,>=1.33 (from langchain)\n","  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n","Collecting langchain-community<0.1,>=0.0.20 (from langchain)\n","  Downloading langchain_community-0.0.20-py3-none-any.whl (1.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting langchain-core<0.2,>=0.1.22 (from langchain)\n","  Downloading langchain_core-0.1.23-py3-none-any.whl (241 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.2/241.2 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting langsmith<0.1,>=0.0.83 (from langchain)\n","  Downloading langsmith-0.0.92-py3-none-any.whl (56 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n","Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.1)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n","Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n","  Downloading marshmallow-3.20.2-py3-none-any.whl (49 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n","  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n","Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.22->langchain) (3.7.1)\n","Collecting langsmith<0.1,>=0.0.83 (from langchain)\n","  Downloading langsmith-0.0.87-py3-none-any.whl (55 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.22->langchain) (23.2)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.2)\n","Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.9.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.22->langchain) (1.3.0)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.22->langchain) (1.2.0)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n","  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n","Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, jsonpatch, langsmith, dataclasses-json, langchain-core, langchain-community, langchain\n","Successfully installed dataclasses-json-0.6.4 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.7 langchain-community-0.0.20 langchain-core-0.1.23 langsmith-0.0.87 marshmallow-3.20.2 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"]}]},{"cell_type":"code","source":["! pip install openai"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OXZYWVdJ0rkB","executionInfo":{"status":"ok","timestamp":1708279240573,"user_tz":-330,"elapsed":11216,"user":{"displayName":"alina shaikh","userId":"07427893967666901792"}},"outputId":"7e95e5a8-e8ef-42ad-da8e-d194690e119a"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting openai\n","  Downloading openai-1.12.0-py3-none-any.whl (226 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n","Collecting httpx<1,>=0.23.0 (from openai)\n","  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.1)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n","Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.9.0)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n","Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n","  Downloading httpcore-1.0.3-py3-none-any.whl (77 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.2)\n","Installing collected packages: h11, httpcore, httpx, openai\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","llmx 0.0.15a0 requires cohere, which is not installed.\n","llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.3 httpx-0.26.0 openai-1.12.0\n"]}]},{"cell_type":"code","source":["! pip install tiktoken"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"INRYAj780vVK","executionInfo":{"status":"ok","timestamp":1708279251754,"user_tz":-330,"elapsed":11198,"user":{"displayName":"alina shaikh","userId":"07427893967666901792"}},"outputId":"d99d52ac-aade-4a89-c41c-c3fa9a9bb346"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tiktoken\n","  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n","Installing collected packages: tiktoken\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed tiktoken-0.6.0\n"]}]},{"cell_type":"code","source":["# generic_template='''\n","# Write a summary of the following Abstract :\n","# Abstract Summary : `{abstract}`\n","# Translate the precise summary to {language}.\n","\n","# '''\n","# prompt=PromptTemplate(\n","#     input_variables=['abstract','language'],\n","#     template=generic_template\n","# )"],"metadata":{"id":"uscZnVUT0z_R","executionInfo":{"status":"ok","timestamp":1708279251754,"user_tz":-330,"elapsed":6,"user":{"displayName":"alina shaikh","userId":"07427893967666901792"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["! pip install PyPDF2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S_fixfHB077s","executionInfo":{"status":"ok","timestamp":1708279262775,"user_tz":-330,"elapsed":11026,"user":{"displayName":"alina shaikh","userId":"07427893967666901792"}},"outputId":"2f9bed7f-021e-4619-b786-91f00c1cd394"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting PyPDF2\n","  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/232.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: PyPDF2\n","Successfully installed PyPDF2-3.0.1\n"]}]},{"cell_type":"code","source":["from langchain import PromptTemplate\n","from langchain.chat_models import ChatOpenAI\n","from langchain.chains.summarize import load_summarize_chain\n","from langchain.text_splitter import RecursiveCharacterTextSplitter"],"metadata":{"id":"vUPYlf7at1D_","executionInfo":{"status":"ok","timestamp":1708279265525,"user_tz":-330,"elapsed":2766,"user":{"displayName":"alina shaikh","userId":"07427893967666901792"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["from langchain.chains import LLMChain\n","from langchain import PromptTemplate"],"metadata":{"id":"ucpxVZjb2kCm","executionInfo":{"status":"ok","timestamp":1708279265526,"user_tz":-330,"elapsed":5,"user":{"displayName":"alina shaikh","userId":"07427893967666901792"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["from PyPDF2 import PdfReader"],"metadata":{"id":"b987y_Se08ky","executionInfo":{"status":"ok","timestamp":1708279265526,"user_tz":-330,"elapsed":4,"user":{"displayName":"alina shaikh","userId":"07427893967666901792"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# provide the path of  pdf file/files.\n","pdfreader = PdfReader('opencv.pdf')"],"metadata":{"id":"Aaw_tflv0978","executionInfo":{"status":"ok","timestamp":1708279313216,"user_tz":-330,"elapsed":4,"user":{"displayName":"alina shaikh","userId":"07427893967666901792"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["from typing_extensions import Concatenate\n","# read text from pdf\n","text = ''\n","for i, page in enumerate(pdfreader.pages):\n","    content = page.extract_text()\n","    if content:\n","        text += content"],"metadata":{"id":"cWNVhl3Y1KbK","executionInfo":{"status":"ok","timestamp":1708279315260,"user_tz":-330,"elapsed":1423,"user":{"displayName":"alina shaikh","userId":"07427893967666901792"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["text"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":209},"id":"1u0t34GC1Lxy","executionInfo":{"status":"ok","timestamp":1708279315260,"user_tz":-330,"elapsed":6,"user":{"displayName":"alina shaikh","userId":"07427893967666901792"}},"outputId":"d09beba7-12dd-4c9b-82c2-e19ec9c12bc6"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'See discussions, st ats, and author pr ofiles f or this public ation at : https://www .researchgate.ne t/public ation/353326963\\nARTIFICIAL INTELLIGENCE IN COMPUTER VISION\\nArticle  · July 2021\\nDOI: 10.33564/IJEAS T.2021. v06i01.037\\nCITATIONS\\n4READS\\n10,186\\n1 author:\\nAryan Karn\\nMotilal Nehru National Instit ute of T echnolog y\\n3 PUBLICA TIONS \\xa0\\xa0\\xa04 CITATIONS \\xa0\\xa0\\xa0\\nSEE PROFILE\\nAll c ontent f ollo wing this p age was uplo aded b y Aryan Karn  on 18 July 2021.\\nThe user has r equest ed enhanc ement of the do wnlo aded file.                      International Journal of Engineering Applied Sciences and Technology , 2021    \\n    Vol. 6, Issue 1, ISSN No. 2455 -2143, Pages 249-254 \\n                                  Published Online May 2021 in IJEAST (http://www.ijeast.com)                                                                                                                                                                                                                                                                                                                                                                                                                                      \\n \\n249 \\n \\nARTIFICIAL INTELLIGENCE IN \\nCOMPUTER VISION  \\nAryan Karn  \\nMotilal Nehru National Institute of Technology Allahabad, Prayagraj  \\nDepartment of Electronics and Communication Engineering  \\n \\nAbstract - Computer vision is an area of research concerned \\nwith assisting computers in seeing. Computer vision issues aim \\nto infer something about the world from observed picture data \\nat the most abstract level. It is a multidisciplinary subject that \\nmay be loosely classified as a branch of artificial intelligence  \\nand machine learning, both of which may include using \\nspecific techniques and using general -purpose learning \\nmethods.  As an interdisciplinary field of research, it may seem \\ndisorganized, with methods taken and reused from various \\nengineering and computer science disciplines. While one \\nspecific vision issue may be readily solved with a hand -crafted \\nstatistical technique, another may need a vast and \\nsophisticated ensemble of generic machine learning \\nalgorithms . Computer vision as a discipline is at the cutting \\nedge of science. As with any frontier, it is thrilling and chaotic, \\nwith often no trustworthy authority to turn to. Numerous \\nbeneficial concepts lack a theoretical foundation, and some \\ntheories are render ed ineffective in reality; developed regions \\nare widely dispersed, and often one seems totally unreachable \\nfrom the other.  \\nKeywords —Computer vision, Artificial intelligence, Neural networks, \\nCNN., Deep learning, machine learning  \\nI. INTRODUCTION  \\n     Recently, computer vision has gained traction and popularity \\nas a consequence of the many applications it has found in areas \\nlike health and medical, sports and entertainment, automaton \\ndesign, and self -driving cars. Many of these applications rely on  \\nvisual recognition tasks such as image order, restriction, and \\nidentification. Recent advances in Convolutional Neural Networks \\n(CNNs) have resulted in an extraordinary performance in these \\nbest-in-class visual recognition assignments and frameworks, \\ndemo nstrating the power of Convolutional Neural Networks. \\nConsequently, convolutional neural networks (CNNs) have \\nemerged as the basic building blocks of deep learning \\ncomputations in computer vision.  \\nDeep Neural Networks (DNN) is a kind of neural network that  has \\nbetter image identification skills and is often utilized in computer \\nvision computations. Convolutional Neural Networks (CNN or \\nConvNet) is a subtype of Deep Neural Networks (DNNs) that are often employed in visual sign decoding. In addition, it is us ed in \\nComputer Vision and Natural Language Processing to organize \\nmaterial (NLP). It is possible to construct a convolutional neural \\nnetwork using a variety of structural blocks. These structural \\nblocks include convolution layers, pooling layers, and fully  \\nconnected layers, all of which will be discussed briefly in this \\narticle. In the next sections, the author covers Deep Learning and \\nthe many neural network techniques lumped together. In addition, \\nthe book covers Convolutional Neural Networks, their \\nconst ruction, and their applications in several fields, including \\nmedicine and engineering.  \\nII. LITERATURE SURVEY  \\nA. Deep Learning and Neural Networks  \\nMachine Learning is a subset of Deep Learning, a subset of \\nArtificial Intelligence (AI). Machine learning uses  algorithms and \\ntraining data to automatically detect patterns and with little human \\nintervention. Artificial Intelligence is a method for teaching \\ncomputers to act like humans. At the same time, Deep Learning is \\ninspired by the structure and function of t he human brain, as \\nrepresented symbolically by an artificial neural network. [12] \\nWhile deep learning was originally proposed in the 1980s, it has \\nshown significant benefits in recent years for two primary reasons:  \\nA. This requires a significant level of k nowledge. For instance, the \\ndevelopment of autonomous vehicles necessitates the collection of \\nmany pictures and lengthy video recordings.  \\nB. Deep learning requires a large capacity for recording. High -\\nperformance GPUs offer an efficient parallel design tha t is well -\\nsuited for deep learning. When used in conjunction with clusters \\nor cloud computing, this significantly lowers the time required to \\ntrain a deep learning network from weeks to hours or less. [11]. \\nDeep learning may be used to solve a wide range o f problems. For \\nexample, the author discusses autonomous driving, aerospace and \\nmilitary, medical research, industrial automation, and electronics \\nin more detail in the closing part of the article.  \\nIn general, a Neural Network is a kind of algorithm that a ccepts \\ncertain input parameters and processes them using an Activation \\nFunction to get the desired Output. In this method, the input to                       International Journal of Engineering Applied Sciences and Technology , 2021    \\n    Vol. 6, Issue 1, ISSN No. 2455 -2143, Pages 249-254 \\n                                  Published Online May 2021 in IJEAST (http://www.ijeast.com)                                                                                                                                                                                                                                                                                                                                                                                                                                      \\n \\n250 \\n \\noutput processing component is referred to as a neuron. Consider \\nthe fundamental example of calculating the purchase pri ce of a \\nhome. Numerous factors must be taken into account, each of \\nwhich has an impact on the Price component. For instance, the \\nsquare footage of the room, the number of bedrooms, and the zip \\ncode. Thus, if we take price as an Output, the following Neural  \\nNetwork shows how a neural network could produce that Output \\nusing the parameters stated earlier as inputs.  \\n \\nFig 1: An example of Standard Neural Network [1]  \\nEach circle represents a neuron that is given an Activation \\nFunction that computes the desired Output by combining distinct \\nvalues for various input parameters. The Activation Function is \\ndetermined by the algorithm\\'s purpose/application. For instance, \\neach circle represents a neuron that is given an Activation \\nFunction that computes the desired Output by combining distinct \\nvalues for various input parameters. The Activation Function is \\ndetermined by the algorithm\\'s purpose/application. For instance, \\nin the  above example, the objective is to determine the maximum \\nprice of a house. For the sake of simplicity, let us suppose that the \\nOutput is solely dependent on two input variables, namely the size \\nand number of bedrooms. In this instance, the bigger the hous e and \\nthe more bedrooms, the greater the price of the house. Thus, the \\nActivation Function (Neuron) will be defined in such a manner \\nthat it will pick the greatest possible value for each input parameter \\nand then compute the Output. Obviously, this seems t o be very \\neasy in this case, but when a large number of factors are involved, \\ndecision -making is not as straightforward as it appears based on \\nmaximum or minimum values alone. And here is where Data -\\nDriven Machine Learning comes into play. The method takes  \\nadvantage of data saved (learned!) from previous instances in \\norder to calculate the optimal Output using the Activation \\nFunction. The above example shows a Standard Neural Network, \\nwhich is often used to generate Output from statistical, numerical, \\nand o ther quantitative data. The kind of Neural Network to employ \\nis determined by the nature of the input data that the algorithm \\nmust handle. The following table summarizes the capabilities of \\ndifferent Neural Networks in processing various kinds of input \\ndata. [1] In the remainder of this article, the author will concentrate \\nonly on the Convolutional Neural Network method used in Deep \\nLearning.  \\nB. Deep Learning using Convolutional Neural Network for \\nComputer Vision       In deep learning, a convolutional neura l network (CNN), often \\nknown as a ConvNet, is a kind of deep neural network that is \\nfrequently used to analyze visual pictures. In certain areas, it is \\nalso referred to as a convolutional neural network (CNN). These \\nartificial neural networks are referred to as shift -invariant artificial \\nneural networks or space -invariant artificial neural networks due \\nto their shared -weights architecture and translation invariance \\nproperties (SIANNs). Algorithms may be used to identify pictures \\nand videos, create recommend er systems, categorize images, do \\nmedical image analysis, and evaluate natural language. In the next \\npart, the author discusses what Convolution is, how it extracts data \\nfrom pictures, and the architecture and components of CNN, \\namong other things. This wi ll show how CNN examines the \\ncontent of an image and processes the data in order to provide the \\nintended result to the audience.  \\nC. Architectural Overview  \\nConvolution is a mathematical procedure that takes two functions \\nand produces a third function that illustrates how the shape of one \\nfunction is affected by the shape of the other. To complete the \\noperation, the Convolution process requires the calculation of the \\nResult function, as well as the initialization of the Result function. \\nConvolution is a data  processing technique that entails \\ncategorizing the components (content) of an image in order to \\nassist Machine Learning and ultimately generate the desired \\nOutput through the algorithm. It is utilized in the processing of \\npicture data. Deep Learning and N eural Networks are two \\ndifferent types of neural networks that are capable of analyzing \\nimage data. Deep Learning is a kind of neural network that enables \\ndata-driven learning. As indicated by the procedure\\'s name, the \\nconvolution process separates the whe at from the chaff.  \\nThis structure may be seen as a three -dimensional volume of \\nneurons in a cellular environment. A distinguishing feature of how \\nCNNs have evolved from earlier feed -forward versions is their \\nability to improve computational efficiency via the addition of \\nnew layer types to their design. How about we take a closer look \\nat the general design of CNNs right now? [4]  \\nD. Basic CNN components  \\n1. Convolutional Layer:  \\n     CNN, or convolutional neural network, is a kind of neural \\nnetwork model that is designed for dealing with two -dimensional \\nimage data, although it may also be used to deal with one -\\ndimensional and three -dimensional data. Convolution is \\naccomplished via the use of a channel (a small matrix whose size \\nmay be chosen). In this channel, which travels the whole picture \\nnetwork, the task is to reproduce the image\\'s features by utilizing \\nthe pixel values that were first used. Each of these increases is \\nadded together to form a single number towards the end of the \\nprocess. When doing a compar ison action, the channel moves \\n                      International Journal of Engineering Applied Sciences and Technology , 2021    \\n    Vol. 6, Issue 1, ISSN No. 2455 -2143, Pages 249-254 \\n                                  Published Online May 2021 in IJEAST (http://www.ijeast.com)                                                                                                                                                                                                                                                                                                                                                                                                                                      \\n \\n251 \\n \\nrightward by n units (this number may vary). After traversing over \\neach place, a framework is produced that is much less in size than \\nthe information grid that was previously constructed. [7]  \\n \\n \\nFig 3: Architecture of a CNN  [6] \\nEdge Detection Example:  \\n     In Figures 4 and 5 below, to detect the horizontal and vertical \\nimages with the help of a matrix, let\\'s consider a greyscale image, \\nwith a 6 x 6 matrix and a filter of 3x3 applied to it.[14]  \\n \\nFig 4: Identifying the edges [14] \\n \\nFig 5: Scanning of the image pattern [14]  \\nAfter the above calculations of the matrix, we get the matrix as \\nshown in fig:6. To calculate it, we take the initial 3 X 3 framework \\nfrom the 6 X 6 picture and increase it with the channel. Let’s  \\nconsider t he following matrix of 4x4 order and the calculation \\ntakes place as: for example,  3*1 + 0 + 1* -1 + 1*1 + 5*0 + 8* -1 + \\n2*1 + 7*0 + 2* -1 = - 5. To compute the second component of the \\n4 X 4 order, we will move the channel one step ahead to the right \\nside of t he original Greyscale matrix and so on: [14]  \\nFig 6:  Matrix calculation in convolutional layer [ 14]       \\n \\n \\n  \\nFig 7: Convolving over the entire image [ 14] \\nThe way to detect the vertical edge in the image is to look for the \\npixel values as, if the pixel values are greater, then brightness at \\nthat part of the image will be more, and if the value is less, it will \\nbe dark. [ 14] \\n2. Pooling Layer:  \\n     Spatial poo ling (alternatively referred to as subsampling or \\ndown sampling ) lowers the dimensionality of each element map \\nwhile preserving the most critical data. Spatial pooling may occur \\nin a number of ways: Quantifiers include the terms maximum, \\naverage, and total . If Max Pooling occurs, we define a spatial \\nneighborhood  (for example, a 22 -window neighborhood ) and \\nchoose the biggest component from the redressed highlight map \\ncontained inside that neighborhood . Rather than choosing the \\nbiggest component, we might cho ose the average (Average \\nPooling) or a total of all components included inside that window. \\nMax Pooling has been shown to be increasingly effective with \\ntime. [8] Max pooling, as shown below, chooses the component \\nwith the largest size from the rectified f eature map. Choosing the \\nbiggest component is equal to using the conventional pooling \\nmethod. The phrase \"sum pooling\" refers to the gathering of all \\ncomponents in an element map. [8]  \\n                      International Journal of Engineering Applied Sciences and Technology , 2021    \\n    Vol. 6, Issue 1, ISSN No. 2455 -2143, Pages 249-254 \\n                                  Published Online May 2021 in IJEAST (http://www.ijeast.com)                                                                                                                                                                                                                                                                                                                                                                                                                                      \\n \\n252 \\n \\n     \\nFig 8: Max Pooling [8]  \\n3. Fully Connected Layers:  \\n     Fully Linked layers have every neuron in the layer above it \\nconnected to every neuron  in the layer below it. To put it simply, \\nFC works in the same manner as a conventional neural network, \\nsuch as a Multi -Layer Perceptron, does (MLP). The main \\ndistinction is that information sources would be molded  and \\norganized in the manner defined by earlier phases of a CNN, rather \\nthan the other way around. [7]. As illustrated in the diagram below, \\nthe feature map matrix is converted into a vector as(x1,x2,...xn) by \\nutilizing the FC layer, and the resulting vectors are merged to \\ncreate a model. Then, using the activation function, we can \\nclassify the Output into different categories.  \\n \\n \\n                                                 \\nFig 9: Fully connected Layer [ 7] \\nIII APPLICATIONS  \\nA. HealthCare:  \\n   Computer vision is extensively employed in the diagnosis of \\ndiseases by analyzing X -rays, magnetic resonance imaging (MRI), \\nand other medical pictures. It has been shown to be just as \\nconvincing as traditional human speciali sts in the area when it \\ncomes to accuracy. On a regular basis, Computer Vision is \\neffectively diagnosing pneumonia, cerebrum tumor’s , diabetes, Parkinson\\'s illness, malignant uterine growth, and a host of other \\nmedical issues, and the technology is getting  more advanced. With \\nthe use of best -in-class image processing technology and \\ncomputer vision methods, early identification of any potential \\ndiseases will be feasible. In this manner, treatment may be \\nadministered at an inconvenient time during the disease  or, in any \\nevent, the likelihood of their recurring is decreased [2].  \\n \\nFig 10: Cross -section of the 3D image of CT Scan and MRI [2]  \\n \\nB. Automobile:  \\n     With the expanded publicity of oneself driving autos, car \\nbusinesses are vigorously subject to Computer Vision since it is \\nintended for understanding the driving condition, including \\nidentifying impediments, people on footpaths, and conceivable \\ncrash ways. Self -driving autos are gradually advancing into the \\nmarket, with more organizations searching for imaginative \\napproaches to bring progressively electric vehicles onto the street. \\nComputer Vision innovation enables these self -driving vehicles \\'to \\nsee\\' the earth while AI calculations make the \"minds\" that help that \\nComputer Vision translate the items around the vehicle. Self -\\ndriving autos are furnished with numerous cameras to give a total \\n360-degree perspective on nature inside the scope of several \\nmeters. Tesla vehicles, for example, utilize something like 8 \\nencompasses cameras to accomplish this accomplishment. Twelve \\nultrasonic sensors for identifying hard and delicate articles out and \\nabout and a front -oriented radar that empowers the identification  \\nof different vehicles even through downpour or mist are \\nadditionally introduced to supplement the cameras. With a lot of \\ninformation being encouraged into the vehicle, a basic PC won\\'t \\nbe sufficient to deal with the inundation of data. This is the reason \\nall self -driving autos have a locally available PC with Computer \\nVision highlights made through AI. The cameras and sensors are \\nentrusted to both recognize and group protests in nature - like \\npeople on foot. The area, thickness, shape, and profundity of th e \\nitems must be considered quickly to empower the remainder of the \\ndriving framework to settle on proper choices. Every one of these \\ncalculations is just conceivable through the incorporation of AI \\nand deep neural systems, which results in highlights like the \\nperson on foot recognition [15].     \\n                      International Journal of Engineering Applied Sciences and Technology , 2021    \\n    Vol. 6, Issue 1, ISSN No. 2455 -2143, Pages 249-254 \\n                                  Published Online May 2021 in IJEAST (http://www.ijeast.com)                                                                                                                                                                                                                                                                                                                                                                                                                                      \\n \\n253 \\n \\n \\n                                                    \\nFig 11: Tesla Car’s Vision, Source: Tesla [ 15] \\nC. Astronomy:  \\n     Our full understanding of the universe is based on photon \\nestimations, which are mostly compose d of pictures of the \\nuniverse. This opens the door to the potential of utilizing \\nComputer Vision in astronomy since our universe is so enormous, \\nand our universe\\'s lone natural rule predicts that the data gathered \\nwill be just as large. It will be impossib le for the stargazer, or for \\nanybody else, to physically contemplate this information in its \\nentirety. We can decipher all of the data in a short period of time, \\nthanks to Computer Vision. To put it another way, computer vision \\nis currently being utilized to find new planets and big bodies, with \\napplications such as exoplanet imaging, star and cosmic system \\ngrouping, and other similar tasks [3].  \\nD. Industrial:  \\n     In Industries, Computer Vision is utilized on the mechanical \\nproduction systems for checking groups, identifying harmed parts, \\nfor the examination of the completed merchandise. Here, Machine \\nVision apparatuses help in discovering infinitesimal level \\nsurrenders in items that basically can\\'t be distinguished through \\nhuman vision. In assembling under takings, perusing scanner tags \\nor QR code are fundamental as they give a one of a kind \\nrecognizable proof to an item. Perusing a great many standardized \\nidentifications in a day isn\\'t a simple errand for people; at the same \\ntime, it very well may be done e ffectively in minutes through \\nComputer Vision [ 3]. \\nIV ConvNet ARCHITECTURE  \\nConvolutional Neural Networks (CNNs) is a kind of neural \\nnetwork that has been around since the mid -90s. You\\'ll discover \\nsome more visually arresting designs in the section below [ 9]. \\nA convolutional neural network was in the process of being \\ncreated from the late 1990s to the middle of the 2010s, and it was \\nknown as LeNet during that time period. The tasks that \\nconvolutional neural networks were capable of doing grew increasingly f ascinating as an ever -increasing quantity of \\ninformation and processing power became accessible.  \\n(2).AlexNet (2012) – In 2012, Alex Krizhevsky (together with \\nothers) published AlexNet, which was a more in -depth and much \\nmore complete version of the LeNet. AlexNet was the clear winner \\nof the inaugural ImageNet Large Scale Visual Recognition \\nChallenge (ILSVRC) in 2012, outperforming the competition by \\na wide margin. This research represented a major advancement \\nover prior techniques, and the widespread use of  CNNs today may \\nbe linked back to the findings of this study.  \\nA Convolutional Network created by Matthew Zeiler and Rob \\nFergus was presented at the ILSVRC 2013 as part of the 3.ZF  Net \\n(2013) session. The ZDNet was the moniker that was given to this \\nnetwork (short for Zeiler and Fergus Net). It was feasible to make \\nimprovements to AlexNet by altering the design hyperparameters \\nused in its creation. When Szegedy and colleagues from G oogle \\npresented a Convolutional Network at the ILSVRC 2014 \\nconference, it was given the moniker \"GoogleLeNet\" (2014). This \\norganization\\'s main goal was the creation of an Inception Module \\nthat significantly decreased the number of parameters in the \\nsystem (4M, contrasted with AlexNet with 60M).  \\nIn the 2014 International Laser Scanning and Vision Research \\nConference (ILSVRC), a system that became known as the \\nVGGNet  was the first to cross the finish line. In particular, it aimed \\nat demonstrating how important it is for efficient execution to have \\na system with sufficient depth (i.e., layers). It was ResNets (2015), \\na Residual Network created by Kaiming He (and others ), that was \\nawarded sixth place in the ILSVRC 2015. ResNets (2015) was the \\nwinner of the ILSVRC 2015. Convolutional Neural Network \\nmodels such as ResNets are currently by a wide margin the best -\\nin-class models, and they will continue to be the default opti on for \\nutilizing ConvNets for the foreseeable future (as of May 2016).  \\nThe seventh source is DenseNet, which was launched in August \\n2016. A network of nodes that are closely packed together. This \\ndensely linked convolutional network, developed by Gao Huang  \\n(and others) and published recently, has each layer directly \\nconnected to every other layer in a feed -forward architecture, with \\neach layer being straightforwardly correlated with each other \\nlayer. Following the completion of five highly concentrated arti cle \\nacknowledgment benchmark assignments, the DenseNet was \\nfound to have gained substantial gains over prior best -in-class \\narchitectures, results revealed. View this video to see exactly how \\nthe Torch was carried out.  \\nV. CONCLUSION  \\n     At the beginning of  the paper, we discussed the overview of \\ndeep learning and how Neural networks in dep learning are \\ndeployed to process various inputs to gain desired outputs.  In the \\nlater part, the author has focused on Convolutional Neural \\n                      International Journal of Engineering Applied Sciences and Technology , 2021    \\n    Vol. 6, Issue 1, ISSN No. 2455 -2143, Pages 249-254 \\n                                  Published Online May 2021 in IJEAST (http://www.ijeast.com)                                                                                                                                                                                                                                                                                                                                                                                                                                      \\n \\n254 \\n \\nNetwork and explained in detai l a convolution operation, the \\nsystem architecture of CNN, and how the layers of CNN work in \\ncoordination to identify the highlights and the patterns of an \\nimage. Using these algorithms author has described how CNN can \\nbe applied in various industries. Thr ough this paper, it can be \\nconcluded that CNN has become a very powerful tool in machine \\nlearning. By providing various images as input data at the machine \\nlearning phase can facilitate the learning process faster, and the \\ndata can be deployed for multiple -output functions, which is a \\nmajor advantage of CNN. Apart from the application, the author \\nmentioned in the earlier section that CNN is now also being \\nconsidered for IoT, Commercial, and domestic security systems. \\nThus, CNN has gained a very prominent pl ace in Data Engineering \\nand still is gaining.  \\nVI REFERENCES  \\n[1] Pulkit Sharma, Analytics Vidhya, An Introductory Guide to \\nDeep Learning and Neural Networks, 22 Oct. 2018, \\nhttps://www.analyticsvidhya.com/blog/2018/10/introduction -\\nneural -networks -deep-learni ng/ \\n[2] Khan, S. A Guide to Convolutional Neural Networks for \\nComputer Vision. Morgan &amp; Claypool, 2018.  \\n[3] Verma, Shiva. “Understanding 1D and 3D Convolution Neural \\nNetwork: Keras.” Medium, Towards Data Science, 1 Oct. 2019,  \\ntowardsdatascience.com/u nderstanding -1d-and-3d-convolution -\\nneural -network -keras -9d8f76e29610.  \\n[4] Upadhyay, Yash. “Computer Vision: A Study on Different \\nCNN Architectures and Their Applications.” Medium, AlumnAI \\nAcademy, 29 Mar. 2019, \\nmedium.com/alumnaiacademy/introduction -to-computer -vision -\\n4fc2a2ba9dc  \\n[5]CS231n Convolutional Neural Networks for Visual \\nRecognition, cs231n.github.io/convolutional -networks/.  \\n[6] “Autopilot.” Tesla, Inc, www.tesla.com/autopilot.  \\n[7] Deshpande, Adit. “A Beginner\\'s Guide To Understanding \\nConvolutiona l Neural Networks.”  \\nA Beginner\\'s Guide To Understanding Convolutional Neural \\nNetworks – Adit Deshpande – Engineering at forwarding | UCLA \\nCS \\'19,  \\nadeshpande3.github.io/adeshpande3.github.io/A -Beginner\\'s -\\nGuide -To-Understanding -Convolutional -Neural -Network s/.  \\n[8] Upadhyay, Yash. “Computer Vision: A Study On Different \\nCNN Architectures and Their Applications.” Medium, AlumnAI \\nAcademy, 29 Mar. 2019, medium.com/alumnaiacademy/introduction -to-computer -vision -\\n4fc2a2ba9dc.  \\n[9] Ujjwalkarn. “An Intuitive Explanat ion of Convolutional \\nNeural Networks.” The Data Science Blog, 29 May 2017, \\nhttps://www.ujjwalkarn.me/2016/08/11/intuitive -explanation -\\nconvnets/.  \\n[10] Gibson, Adam, and Josh Patterson. “Deep Learning.” \\nO\\'Reilly | Safari, O\\'Reilly Media, Inc.,   \\nwww.oreilly .com/library/view/deep -\\nlearning/9781491924570/ch04.html  \\n[11] “What Is Deep Learning?: How It Works, Techniques &amp; \\nApplications.” How It Works, Techniques &amp; Applications - \\nMATLAB &amp; Simulink, \\nwww.mathworks.com/discovery/deep -learning.html.  \\n[12] Br ownlee, Jason. “What Is Deep Learning?” Machine \\nLearning Mastery, 31 Oct. 2019, \\nmachinelearningmastery.com/what -is-deep-learning/.  \\nView publication stats'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"10Wyz4IE1k3I","executionInfo":{"status":"ok","timestamp":1708279315707,"user_tz":-330,"elapsed":452,"user":{"displayName":"alina shaikh","userId":"07427893967666901792"}},"outputId":"0273ee45-71e4-476a-bf96-30409197eb3c"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n","  warn_deprecated(\n"]}]},{"cell_type":"code","source":["llm.get_num_tokens(text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-bthDus31leG","executionInfo":{"status":"ok","timestamp":1708279316416,"user_tz":-330,"elapsed":713,"user":{"displayName":"alina shaikh","userId":"07427893967666901792"}},"outputId":"e2b3cf38-84ff-4f99-f1c2-de773a3744bb"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["6098"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["## Splittting the text\n","text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=20)\n","chunks = text_splitter.create_documents([text])"],"metadata":{"id":"C1pL0rZE1ngW","executionInfo":{"status":"ok","timestamp":1708279316417,"user_tz":-330,"elapsed":12,"user":{"displayName":"alina shaikh","userId":"07427893967666901792"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["len(chunks)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t1nf1YZ01rxd","executionInfo":{"status":"ok","timestamp":1708279316417,"user_tz":-330,"elapsed":11,"user":{"displayName":"alina shaikh","userId":"07427893967666901792"}},"outputId":"42a4e151-77dc-426c-fb80-360554d23e3c"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["chunks_prompt=\"\"\"\n","Write a summarize the given Research Paper:\n","Speech:`{text}'\n","Summary:\n","\"\"\"\n","map_prompt_template=PromptTemplate(input_variables=['text'],\n","                                    template=chunks_prompt)"],"metadata":{"id":"YdBT4QOo13Ax","executionInfo":{"status":"ok","timestamp":1708279316417,"user_tz":-330,"elapsed":8,"user":{"displayName":"alina shaikh","userId":"07427893967666901792"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["final_combine_prompt='''\n","Provide a final summary of the entire Research Paper with these important points.\n","Add business analytics and real worls use cases,\n","Start the precise summary with an introduction and provide the\n","summary in number points for the Research Paper.\n","Speech: `{text}`\n","'''\n","final_combine_prompt_template=PromptTemplate(input_variables=['text'],\n","                                             template=final_combine_prompt)"],"metadata":{"id":"LGZ4K6fF2O_N","executionInfo":{"status":"ok","timestamp":1708279316417,"user_tz":-330,"elapsed":7,"user":{"displayName":"alina shaikh","userId":"07427893967666901792"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["summary_chain = load_summarize_chain(\n","    llm=llm,\n","    chain_type='map_reduce',\n","    map_prompt=map_prompt_template,\n","    combine_prompt=final_combine_prompt_template,\n","    verbose=False\n","\n",")\n","output = summary_chain.run(chunks)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7MnCgKcO2WOm","executionInfo":{"status":"ok","timestamp":1708279344704,"user_tz":-330,"elapsed":27439,"user":{"displayName":"alina shaikh","userId":"07427893967666901792"}},"outputId":"b5ca0418-00c1-43cf-c1dd-70fb8fe7a074"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n","  warn_deprecated(\n"]}]},{"cell_type":"code","source":["output"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":244},"id":"7tnBFGGo4SVV","executionInfo":{"status":"ok","timestamp":1708279344705,"user_tz":-330,"elapsed":21,"user":{"displayName":"alina shaikh","userId":"07427893967666901792"}},"outputId":"d5ddbe02-05b3-4e9d-fe13-d176ef68430e"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Introduction:\\nThe research paper delves into the use of artificial intelligence in computer vision, specifically focusing on Convolutional Neural Networks (CNNs) and their applications in various industries.\\n\\nSummary:\\n1. The paper explains the basics of neural networks and how CNNs are utilized in computer vision tasks, highlighting their role in enhancing visual recognition.\\n2. It discusses the components of CNNs, such as Convolutional Layers, Pooling Layers, and Fully Connected Layers, and how they process image data for applications in healthcare, autonomous driving, and more.\\n3. The research paper explores the wide range of applications of CNNs in industries like healthcare, aerospace, industrial automation, and astronomy, showcasing their ability to diagnose diseases, interpret surroundings for self-driving cars, and identify new planets.\\n4. It provides a historical overview of the development of CNNs, from LeNet to more recent advancements like AlexNet, ZF Net, GoogleLeNet, VGGNet, ResNets, and DenseNet, emphasizing their evolution and significance in machine learning.\\n5. The conclusion underscores the importance of CNNs in data engineering and their potential applications in IoT, commercial, and domestic security systems, highlighting their role as a powerful tool in various industries. \\n\\nBusiness Analytics and Real-World Use Cases:\\n- CNNs are being used in the healthcare industry for diagnosing diseases from medical images, improving patient outcomes.\\n- In the automotive industry, CNNs are enabling self-driving cars to interpret their surroundings and make informed decisions.\\n- CNNs are utilized in industrial automation for tasks like inspecting manufacturing processes and reading barcodes, enhancing efficiency and accuracy.\\n- In astronomy, CNNs are used to identify new planets and analyze astronomical data, aiding in the discovery of new celestial bodies.\\n- The evolution of CNNs from LeNet to modern architectures like DenseNet showcases their adaptability and effectiveness in various real-world applications.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["# Introduction:\n","# The research paper delves into the use of artificial intelligence in computer vision, specifically focusing on Convolutional Neural Networks (CNNs) and their applications in various industries.\n","\n","# Summary:\n","# 1. The paper explains the basics of neural networks and how CNNs are utilized in computer vision tasks, highlighting their role in enhancing visual recognition.\n","# 2. It discusses the components of CNNs, such as Convolutional Layers, Pooling Layers, and Fully Connected Layers, and how they process image data for applications in healthcare, autonomous driving, and more.\n","# 3. The research paper explores the wide range of applications of CNNs in industries like healthcare, aerospace, industrial automation, and astronomy, showcasing their ability to diagnose diseases, interpret surroundings for self-driving cars, and identify new planets.\n","# 4. It provides a historical overview of the development of CNNs, from LeNet to more recent advancements like AlexNet, ZF Net, GoogleLeNet, VGGNet, ResNets, and DenseNet, emphasizing their evolution and significance in machine learning.\n","# 5. The conclusion underscores the importance of CNNs in data engineering and their potential applications in IoT, commercial, and domestic security systems, highlighting their role as a powerful tool in various industries.\n","\n","# Business Analytics and Real-World Use Cases:\n","# - CNNs are being used in the healthcare industry for diagnosing diseases from medical images, improving patient outcomes.\n","# - In the automotive industry, CNNs are enabling self-driving cars to interpret their surroundings and make informed decisions.\n","# - CNNs are utilized in industrial automation for tasks like inspecting manufacturing processes and reading barcodes, enhancing efficiency and accuracy.\n","# - In astronomy, CNNs are used to identify new planets and analyze astronomical data, aiding in the discovery of new celestial bodies.\n","# # - The evolution of CNNs from LeNet to modern architectures like DenseNet showcases their adaptability and effectiveness in various real-world applications."],"metadata":{"id":"QEde3CXTXfNM","executionInfo":{"status":"ok","timestamp":1708279344705,"user_tz":-330,"elapsed":18,"user":{"displayName":"alina shaikh","userId":"07427893967666901792"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["!pip install transformers -U -q"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ig3dEY2P-_a9","executionInfo":{"status":"ok","timestamp":1708279372403,"user_tz":-330,"elapsed":27716,"user":{"displayName":"alina shaikh","userId":"07427893967666901792"}},"outputId":"d8ef7fc2-6117-4e6d-a578-5efaf7978ad0"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["! pip install sentencepiece"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vO0AWDb-AFbu","executionInfo":{"status":"ok","timestamp":1708279403468,"user_tz":-330,"elapsed":14212,"user":{"displayName":"alina shaikh","userId":"07427893967666901792"}},"outputId":"99323f1e-85c9-4c88-9e52-34846a19fc13"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"]}]},{"cell_type":"code","source":["!pip freeze | grep transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AJyXAIDcAO07","executionInfo":{"status":"ok","timestamp":1708279405259,"user_tz":-330,"elapsed":1808,"user":{"displayName":"alina shaikh","userId":"07427893967666901792"}},"outputId":"37304f6e-af3c-4d04-dc36-26621f008a8e"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["transformers==4.37.2\n"]}]},{"cell_type":"code","source":["from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n"],"metadata":{"id":"62Qf1FHFAR-f","executionInfo":{"status":"ok","timestamp":1708279410285,"user_tz":-330,"elapsed":5030,"user":{"displayName":"alina shaikh","userId":"07427893967666901792"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":237,"referenced_widgets":["6f6641631ccb4515a4e57b3f6f45ef14","47efdae5998a43aebcf154abc26fd862","f6d0982fa594453b987c20bb95a24472","6bf12d71ae804a04b6e385891777cc6e","078ed8146283467db43eb5cf3d99e4c0","4225b7bd84964d94982cf81ab8bcbfcb","87388a23328a42b1bc8fdc3b816ae945","f26ad06a98aa4efba7807d46f59d78c9","eb2fde40ad784cd0999e74cce385e9a3","ee5e92cf0246489e94fae6057d771a02","e5a23c3c46e94eeab18a44b993c6cfe2","bc110421adb54b419938485a68b6b992","2cffea010e774fe6b85bce83951476d6","7f21468929eb439bb37036081dc106b6","4b65a4a950c3445d9110f28998760b20","24ed4e28989c4625aec5d4e88bec4fbd","22b0261d02b6413197852259c7ddc29f","f64582a4e9424b7f98ab14c24cafd72d","d5291805a50a49beb22885d5d83f64d6","7eee3028cac744c9a92e743c24784d14","9019d0a597e74ec598435793e4704496","ce3dbd5d580142878f6946d981dbae63","35d7fd71c90747cfba36977cde0ae5a4","6c1dab3e72be4439b97b60c132b07e25","3f93ecf8e6e34156b1bf0fb4fc069060","68c1c8a8cf5a4360b3b3517a6ac6ff9a","cf5b072ad5f948e095dfa044eebadc41","aaaccd87df0c4a479feedc4c04a8b834","814c453360d54b54b768f4939386d3e2","d04ab1227587484f97e448d05b1085f6","e080548c8b33464884bcbf2d76df5dd9","09d9a411b3b84669ac8ce8909d0b50a2","554cd170af4a4fbdbcd5b24c32a8cea3"]},"id":"I_wlI0LRAT7r","executionInfo":{"status":"ok","timestamp":1708279448531,"user_tz":-330,"elapsed":38251,"user":{"displayName":"alina shaikh","userId":"07427893967666901792"}},"outputId":"277d0f26-658a-4134-f7c6-08581f6a01b9"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f6641631ccb4515a4e57b3f6f45ef14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/2.44G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc110421adb54b419938485a68b6b992"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/261 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35d7fd71c90747cfba36977cde0ae5a4"}},"metadata":{}}]},{"cell_type":"code","source":["tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\", src_lang=\"en_XX\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["60178746f4f641d18e15e34ffff5624c","6871e04fc85e4dd7909fac8f7fdf09b2","9e528916597a4749987bcb2e00b11c46","f470760985774786afdf70938ad49105","30f57fb044cb483f90526a86832d76b9","4ad5ecff80614815b678f251ca433507","e486fb0c065d49d0949f99c441f4a77f","2ba7c61991da4369a579b14ec5d542ab","9a61363cb8fa4c2c83c675a6f5de98cc","10ccc27d7d4148d980fbebfe58bfa726","05f9f3e6c23d4b3bb5d6d2af2715f661","d1d6b39e236b4308abb148f9d543881c","d3f7c0ea641d41c4a4e100c0d3d529f9","b7ede9c7c76c402890dd403c2e089aee","f880d87f7ee04af197d304069864b9bd","ff60ee85e13f45e59ca828a34a1d10a7","08d64eb7d53141a0a55abf22c9773a95","9b12961f9f8c4ca6a7191c2db197a4a5","88e733d4797145acaa2aedf96c837e2e","5c0380c2ff654aa6af8686717ffc617d","961ca24821fb44439e12ee3e2846fc67","f12296c8df59404884aa1bf869fa6587","8c3d9ea08b2a4561b576d56695bc5a2e","e8452a64b8a94619a2b84cc60770b682","9a8f994b778249f9bf1434b5879aee29","ab69c843dc78446fa1529db94897d3be","61db3c657a924b259c4b8897e5cf877c","267350bd5d7c4b0bb9af36950a714076","91d22b361ac14e8cadbb9d199695c454","56ad6a91e88847a2a33e50473bbc0d31","7def6fcca8f14905a103a824b7928502","668d27875b684b0f833c023f4d9d0212","7d38d8024fb54c1f94b20fd0009b8760"]},"id":"NJjf6HP7AXAm","executionInfo":{"status":"ok","timestamp":1708279546855,"user_tz":-330,"elapsed":4734,"user":{"displayName":"alina shaikh","userId":"07427893967666901792"}},"outputId":"7bd93aed-96e9-40c5-fbba-0629533910a4"},"execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/529 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60178746f4f641d18e15e34ffff5624c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1d6b39e236b4308abb148f9d543881c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/649 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c3d9ea08b2a4561b576d56695bc5a2e"}},"metadata":{}}]},{"cell_type":"code","source":["article_en = output"],"metadata":{"id":"_HSIEFFaPAKt","executionInfo":{"status":"ok","timestamp":1708279546856,"user_tz":-330,"elapsed":5,"user":{"displayName":"alina shaikh","userId":"07427893967666901792"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["model_inputs = tokenizer(article_en, return_tensors=\"pt\")\n"],"metadata":{"id":"foGXL4ZwPE7p","executionInfo":{"status":"ok","timestamp":1708279546856,"user_tz":-330,"elapsed":4,"user":{"displayName":"alina shaikh","userId":"07427893967666901792"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["\n","# translate from English to Hindi\n","generated_tokens = model.generate(\n","    **model_inputs,\n","    forced_bos_token_id=tokenizer.lang_code_to_id[\"hi_IN\"]\n",")\n","\n"],"metadata":{"id":"u5gYfz2NPIHr","executionInfo":{"status":"ok","timestamp":1708279685472,"user_tz":-330,"elapsed":138620,"user":{"displayName":"alina shaikh","userId":"07427893967666901792"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["translation = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n"],"metadata":{"id":"9ptrN1EVPL0k","executionInfo":{"status":"ok","timestamp":1708279685472,"user_tz":-330,"elapsed":14,"user":{"displayName":"alina shaikh","userId":"07427893967666901792"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["output"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":244},"id":"HNW3PcXIVBo1","executionInfo":{"status":"ok","timestamp":1708279685472,"user_tz":-330,"elapsed":12,"user":{"displayName":"alina shaikh","userId":"07427893967666901792"}},"outputId":"e81aa37e-07b9-40cc-f087-66f37f785073"},"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Introduction:\\nThe research paper delves into the use of artificial intelligence in computer vision, specifically focusing on Convolutional Neural Networks (CNNs) and their applications in various industries.\\n\\nSummary:\\n1. The paper explains the basics of neural networks and how CNNs are utilized in computer vision tasks, highlighting their role in enhancing visual recognition.\\n2. It discusses the components of CNNs, such as Convolutional Layers, Pooling Layers, and Fully Connected Layers, and how they process image data for applications in healthcare, autonomous driving, and more.\\n3. The research paper explores the wide range of applications of CNNs in industries like healthcare, aerospace, industrial automation, and astronomy, showcasing their ability to diagnose diseases, interpret surroundings for self-driving cars, and identify new planets.\\n4. It provides a historical overview of the development of CNNs, from LeNet to more recent advancements like AlexNet, ZF Net, GoogleLeNet, VGGNet, ResNets, and DenseNet, emphasizing their evolution and significance in machine learning.\\n5. The conclusion underscores the importance of CNNs in data engineering and their potential applications in IoT, commercial, and domestic security systems, highlighting their role as a powerful tool in various industries. \\n\\nBusiness Analytics and Real-World Use Cases:\\n- CNNs are being used in the healthcare industry for diagnosing diseases from medical images, improving patient outcomes.\\n- In the automotive industry, CNNs are enabling self-driving cars to interpret their surroundings and make informed decisions.\\n- CNNs are utilized in industrial automation for tasks like inspecting manufacturing processes and reading barcodes, enhancing efficiency and accuracy.\\n- In astronomy, CNNs are used to identify new planets and analyze astronomical data, aiding in the discovery of new celestial bodies.\\n- The evolution of CNNs from LeNet to modern architectures like DenseNet showcases their adaptability and effectiveness in various real-world applications.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":["translation"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CuRfQd0EPN0u","executionInfo":{"status":"ok","timestamp":1708279685473,"user_tz":-330,"elapsed":10,"user":{"displayName":"alina shaikh","userId":"07427893967666901792"}},"outputId":"5243f682-b3c7-4c09-e10c-5c87685a7ec3"},"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['आरंभः इस शोध पत्र में कंप्यूटर दृश्य में कृत्रिम आकलन के प्रयोग का विस्तार किया गया है, विशेष रूप से कंबल्यूशनल न्यूरल नेटवर्क (सीएनएन) और विभिन्न उद्योगों में उनके अनुप्रयोगों पर ध्यान केंद्रित किया गया है. सारांशः 1. इस पत्र में न्यूरल नेटवर्क की बुनियादी जानकारी दी गई है और कैसे सीएन का प्रयोग कंप्यूटर दृश्य कार्यों में किया जाता है, दृश्य पहचान में उनकी भूमिका को उजागर किया गया है. 2. इसमें सीएन के घटकों पर चर्चा की गई है, जैसे कंबल्यूशनल लेयर, पूलिंग लेयर, और पूरी तरह से जुड़ा लेयर, और वे स्वास्थ्य देखभाल, स्वचालित ड्राइविंग आदि के अनुप्रयोगों के लिए छवि डेटा का संसाधन कैसे करते हैं. 3. इस शोध पत्र में स्वास्थ्य देखभाल, एयरोस्पेस, औद्योगिक स्वचालन और खगोल विज्ञान जैसे उद्योगों में सीएन']"]},"metadata":{},"execution_count":33}]},{"cell_type":"markdown","source":["# Marathi"],"metadata":{"id":"fQuJg7ncPm0x"}},{"cell_type":"code","source":["\n","# translate from  Hindi to Marathi\n","generated_tokens = model.generate(\n","    **model_inputs,\n","    forced_bos_token_id=tokenizer.lang_code_to_id[\"mr_IN\"]\n",")\n","\n"],"metadata":{"id":"Y1tksDyAPmWd","executionInfo":{"status":"ok","timestamp":1708281875477,"user_tz":-330,"elapsed":92099,"user":{"displayName":"alina shaikh","userId":"07427893967666901792"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["translation = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n"],"metadata":{"id":"QnsvVnAqPmP6","executionInfo":{"status":"ok","timestamp":1708281875478,"user_tz":-330,"elapsed":6,"user":{"displayName":"alina shaikh","userId":"07427893967666901792"}}},"execution_count":51,"outputs":[]},{"cell_type":"code","source":["translation"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tgz5H_GZPmGA","executionInfo":{"status":"ok","timestamp":1708281875478,"user_tz":-330,"elapsed":5,"user":{"displayName":"alina shaikh","userId":"07427893967666901792"}},"outputId":"97ad8958-ae31-4cde-a794-422a2a320e90"},"execution_count":52,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['प ् रथम: ह ् या शोधप ् रबंधात कृत ् रिम आकलन क ् रियांवरविश ् वास केला, विशेषतः कंबल ् युशनल न ् युरल नेटवर ् क (सीएन) आणि त ् यांच ् या अनुप्रयोगांबद ् दल.थोडक ् यात. ह ् या शोधप ् रबंधात न ् युरल नेटवर ् क की बुनियादी माहिती दिली गेली आणि सीएन का प्रयोग संगणकीय दृश्य कार ् यक ् रमात केला, त ् यांचा दृष ् टीगत ओळखण ् यात क ् रियांवरचर ् चा केली गेली कंबल ् युशनल लेयर, पूलिंग लेयर, आणि पूर ् ण कन ् टेड ले']"]},"metadata":{},"execution_count":52}]},{"cell_type":"markdown","source":["#Japanese"],"metadata":{"id":"YO9Vc0aVZifY"}},{"cell_type":"code","source":["\n","# translate from English to Japanese\n","generated_tokens = model.generate(\n","    **model_inputs,\n","    forced_bos_token_id=tokenizer.lang_code_to_id[\"ja_XX\"]\n",")\n","\n"],"metadata":{"id":"YLczH3BwZh_I","executionInfo":{"status":"ok","timestamp":1708280183929,"user_tz":-330,"elapsed":34397,"user":{"displayName":"alina shaikh","userId":"07427893967666901792"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["translation = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n"],"metadata":{"id":"O91bE2AaZh29","executionInfo":{"status":"ok","timestamp":1708280183930,"user_tz":-330,"elapsed":19,"user":{"displayName":"alina shaikh","userId":"07427893967666901792"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["translation"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TOxFlYayZhzd","executionInfo":{"status":"ok","timestamp":1708280183930,"user_tz":-330,"elapsed":17,"user":{"displayName":"alina shaikh","userId":"07427893967666901792"}},"outputId":"bcc1ed1b-fcc5-4dc4-ba0c-0a0fefe37a2a"},"execution_count":46,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['まず、この研究では、人工知能の利用について、特に、通信無線ネットワーク(CNN)とその応用について説明しました。 次に、この研究では、通信無線ネットワークの बुनियादी情報と、通信無線ネットワークの利用について、コンピュータビジュアル機能の実現、視覚認識における役割について説明しました。 次に、CNNの構成要素について説明しました。']"]},"metadata":{},"execution_count":46}]},{"cell_type":"markdown","source":["# Russian"],"metadata":{"id":"V82yXlHHZeDz"}},{"cell_type":"code","source":["\n","# translate from English to Hindi\n","generated_tokens = model.generate(\n","    **model_inputs,\n","    forced_bos_token_id=tokenizer.lang_code_to_id[\"ru_RU\"]\n",")\n","\n"],"metadata":{"id":"eRAp1urhZdmw","executionInfo":{"status":"ok","timestamp":1708280236511,"user_tz":-330,"elapsed":52597,"user":{"displayName":"alina shaikh","userId":"07427893967666901792"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["translation = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n"],"metadata":{"id":"yaiUVs4uZxHU","executionInfo":{"status":"ok","timestamp":1708280236511,"user_tz":-330,"elapsed":17,"user":{"displayName":"alina shaikh","userId":"07427893967666901792"}}},"execution_count":48,"outputs":[]},{"cell_type":"code","source":["translation"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WuSY8t3CZxAR","executionInfo":{"status":"ok","timestamp":1708281759007,"user_tz":-330,"elapsed":438,"user":{"displayName":"alina shaikh","userId":"07427893967666901792"}},"outputId":"f582f046-2b49-4ed6-8f5d-b5754e54128b"},"execution_count":49,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Во-первых, в этом исследовании были рассмотрены возможности использования искусственного интеллекта, в частности, для работы с кабельным нейроном (CNN) и его приложениями. Приведены следующие сведения: 1. В этом исследовании были подведены основы работы с кабельным нейроном (CNN) и использованы компьютерные визуальные функции работы с нейронами, их роль в визуальной идентификации. 2. В этом исследовании были обсуждены компоненты CNN, такие как кабельный лейер, пуллинг лейер, полностью концентрированный лейер и так далее.']"]},"metadata":{},"execution_count":49}]},{"cell_type":"code","source":[],"metadata":{"id":"DuV9Szv8hv7G"},"execution_count":null,"outputs":[]}]}